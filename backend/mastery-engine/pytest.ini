[pytest]
# Test Configuration for Mastery Engine

# Async support for pytest-asyncio
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Coverage configuration
addopts =
    --verbose
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=90
    --strict-markers
    --disable-warnings

# Markers
markers =
    unit: Unit tests for individual components
    integration: Integration tests with external services
    contract: API contract tests
    e2e: End-to-end tests
    performance: Performance and load tests
    security: Security-related tests
    slow: Tests that take significant time to run
    dapr: Tests requiring Dapr
    kafka: Tests requiring Kafka
    redis: Tests requiring Redis

# Async test configuration
asyncio_default_fixture_loop_scope = function

# Mocking configuration
mock_use_standalone_module = true

# Output configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File handling
cache_dir = .pytest_cache
log_file = pytest.log
log_file_level = INFO

# Timeout configuration (in seconds)
timeout = 30
timeout_method = thread

# Parallel execution (if xdist is installed)
# addopts = -n auto

# HTML report
addopts = --html=htmlcov/report.html --self-contained-html

# Strict checking for unimplemented tests
strict_markers = true
strict_config = true

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::unclosed.*:ResourceWarning
    error::pytest.PytestWarning

# Test discovery patterns
norecursedirs =
    .git
    __pycache__
    .pytest_cache
    htmlcov
    dist
    build
    .venv
    venv
    env

# Minimum coverage by module
cov_branch = true
cov_source = src/
cov_report =
    term-missing:skip-covered
    html:htmlcov
    xml:coverage.xml

# Xfail strict
xfail_strict = true

# Failure handling
maxfail = 5

# Benchmark configuration (if pytest-benchmark is used)
benchmark_max_time = 0.5
benchmark_disable_gc = false
benchmark_timer = time.perf_counter
benchmark_rounds = 5
benchmark_iterations = 1000